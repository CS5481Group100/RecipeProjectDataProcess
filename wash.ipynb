{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning lines: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1550151/1550151 [01:28<00:00, 17528.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ å®Œæˆï¼å…±å¤„ç† 1544608 æ¡è®°å½•ã€‚è¾“å‡ºæ–‡ä»¶ï¼šrecipes_cleaned_dedup.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import emoji\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# âš™ï¸ å‚æ•°\n",
    "# =========================================================\n",
    "INPUT_FILE = \"recipe_corpus_full.json\"\n",
    "OUTPUT_JSON_FILE = \"recipes_cleaned_dedup.json\"\n",
    "MAX_TEXT_LEN = 1500   # ğŸ”¥ text æœ€ç»ˆæœ€å¤§é•¿åº¦\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ§© emoji ç»Ÿä¸€å¤„ç†\n",
    "# =========================================================\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\" \"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"\n",
    "    \"\\U0001F700-\\U0001F77F\" \"\\U0001F780-\\U0001F7FF\" \"\\U0001F800-\\U0001F8FF\"\n",
    "    \"\\U0001F900-\\U0001F9FF\" \"\\U0001FA00-\\U0001FA6F\" \"\\U0001FA70-\\U0001FAFF\"\n",
    "    \"\\U00002700-\\U000027BF\" \"\\U0001F1E0-\\U0001F1FF\" \"]+\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def replace_emojis(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    prev = {\"last\": None}\n",
    "    def repl(match):\n",
    "        em = match.group(0)[0]\n",
    "        name = emoji.demojize(em, language=\"zh\")\n",
    "        name = re.sub(r'^:+|:+$', '', name)\n",
    "        name = re.sub(r'[_]+', '', name).strip()\n",
    "        if not name or name == em:\n",
    "            name = emoji.demojize(em)\n",
    "            name = re.sub(r'^:+|:+$', '', name)\n",
    "            name = re.sub(r'[_]+', '', name).strip()\n",
    "        if not name:\n",
    "            return ''\n",
    "        if name == prev[\"last\"]:\n",
    "            return ''\n",
    "        prev[\"last\"] = name\n",
    "        return name\n",
    "    out = re.sub(emoji_pattern, repl, s)\n",
    "    out = re.sub(r'\\s+', ' ', out).strip()\n",
    "    return out\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ§¹ æ–‡æœ¬æ¸…æ´—\n",
    "# =========================================================\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.replace(\"\\\\n\", \"\\n\")\n",
    "    s = re.sub(r'[\\b\\r\\t]', '', s)\n",
    "    s = re.sub(r'\\\\\"+', '\"', s)\n",
    "    s = re.sub(r'\"\"+', '', s)\n",
    "    s = re.sub(r';{2,}', ';', s)\n",
    "    s = replace_emojis(s)\n",
    "    s = re.sub(r'^\\s*å›¾ç‰‡\\s*å›¾ç‰‡?\\s*$', '', s, flags=re.MULTILINE)\n",
    "    s = re.sub(r'[^\\u4e00-\\u9fa5A-Za-z0-9ï¼Œã€‚ã€â€œâ€â€˜â€™ï¼ï¼›ï¼šï¼šã€Šã€‹ã€ˆã€‰Â·,.!?()ï¼ˆï¼‰\\s-]', '', s)\n",
    "    s = re.sub(r'[ \\t\\f\\v]+', ' ', s)\n",
    "    s = re.sub(r'\\s*([ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›,.!?()ï¼ˆï¼‰])\\s*', r'\\1', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ§¹ æ¸…ç†åˆ—è¡¨å…ƒç´ \n",
    "# =========================================================\n",
    "def clean_list_item(x: str) -> str:\n",
    "    if not isinstance(x, str):\n",
    "        return x\n",
    "    x = clean_text(x)\n",
    "    if \"æˆå“\" in x.replace(\" \", \"\") or \"çœ‹å›¾æ–‡ä¸­çš„åšæ³•\" in x.replace(\" \", \"\"):\n",
    "        return \"\"\n",
    "    return x\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ§¹ æ¸…ç† name/dish\n",
    "# =========================================================\n",
    "def clean_name(s: str) -> str:\n",
    "    s = clean_text(s)\n",
    "    s = re.sub(r'[\\(\\ï¼ˆ][^\\w\\u4e00-\\u9fa5]*[\\)\\ï¼‰]', '', s)\n",
    "    return s.strip()\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ”¥ æ„å»º textï¼ˆé¡ºåº & æˆªæ–­ 1500 å­—ï¼‰\n",
    "# =========================================================\n",
    "def process_line(line, count):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    obj.pop(\"author\", None)\n",
    "\n",
    "    # æ¸…æ´—æ‰€æœ‰å­—æ®µ\n",
    "    for k, v in list(obj.items()):\n",
    "        if isinstance(v, str):\n",
    "            obj[k] = clean_text(v)\n",
    "        elif isinstance(v, list):\n",
    "            cleaned_list = [clean_list_item(x) for x in v]\n",
    "            obj[k] = [x for x in cleaned_list if x]\n",
    "\n",
    "    # dish\n",
    "    dish = (obj.get(\"dish\") or \"\").strip()\n",
    "    if not dish or dish.lower() == \"unknown\":\n",
    "        dish = obj.get(\"name\", \"\").strip()\n",
    "    dish = clean_name(dish)\n",
    "\n",
    "    parts = []\n",
    "    if dish:\n",
    "        parts.append(f\"dish: {dish}\")\n",
    "\n",
    "    ingredients = obj.get(\"recipeIngredient\", [])\n",
    "    if ingredients:\n",
    "        parts.append(\"recipeIngredient: \" + \" \".join(ingredients))\n",
    "\n",
    "    # recipeInstructions æ”¾æœ€åä¹‹ä¸€\n",
    "    recipe_instructions = obj.get(\"recipeInstructions\", [])\n",
    "    if recipe_instructions:\n",
    "        steps_lines = [f\"{i+1}.{x}\" for i, x in enumerate(recipe_instructions)]\n",
    "        instructions_block = \"recipeInstructions: \" + \" \".join(steps_lines)\n",
    "    else:\n",
    "        instructions_block = \"\"\n",
    "\n",
    "    # keywords æ”¾ instructions å‰\n",
    "    keywords = obj.get(\"keywords\", [])\n",
    "    if keywords:\n",
    "        parts.append(\"keywords: \" + \"ï¼Œ\".join(keywords))\n",
    "\n",
    "    # description æ”¾ instructions å‰ï¼ˆå€’æ•°ç¬¬äºŒï¼‰\n",
    "    description = obj.get(\"description\", \"\")\n",
    "    description_block = f\"description: {description}\" if description else \"\"\n",
    "\n",
    "    # åˆæˆ textï¼šä¿æŒä½ è¦æ±‚çš„é¡ºåº\n",
    "    text_parts = []\n",
    "    text_parts.extend(parts)                # dish, ingredients, keywords\n",
    "    if description_block:\n",
    "        text_parts.append(description_block)  # å€’æ•°ç¬¬äºŒ\n",
    "    if instructions_block:\n",
    "        text_parts.append(instructions_block) # æœ€å\n",
    "\n",
    "    text = \" \".join(text_parts)\n",
    "\n",
    "    # å»æ¢è¡Œä¸å¤šç©ºç™½\n",
    "    text = text.replace(\"\\\\n\", \" \").replace(\"\\n\", \" \")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # ğŸ”¥ TRUNCATE to 1500 characters\n",
    "    if len(text) > MAX_TEXT_LEN:\n",
    "        text = text[:MAX_TEXT_LEN].rstrip()\n",
    "\n",
    "    return {\n",
    "        \"id\": count + 1,\n",
    "        \"name\": obj.get(\"name\", \"\").strip(),\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "\n",
    "def jaccard_similarity(s1, s2):\n",
    "    set1, set2 = set(s1), set(s2)\n",
    "    if not set1 or not set2:\n",
    "        return 0\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ”¥ ä¸»æµç¨‹\n",
    "# =========================================================\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "\n",
    "processed = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(process_line)(line, i) for i, line in enumerate(tqdm(lines, desc=\"Cleaning lines\", ncols=90))\n",
    ")\n",
    "\n",
    "output_list = [x for x in processed if x]\n",
    "\n",
    "output_list_sorted = sorted(output_list, key=lambda x: x['name'])\n",
    "\n",
    "deduped_list = []\n",
    "prev_item = None\n",
    "for item in output_list_sorted:\n",
    "    if prev_item is None:\n",
    "        deduped_list.append(item)\n",
    "        prev_item = item\n",
    "    else:\n",
    "        if prev_item['name'] == item['name']:\n",
    "            sim = jaccard_similarity(prev_item['text'], item['text'])\n",
    "            if sim < 0.8:\n",
    "                deduped_list.append(item)\n",
    "                prev_item = item\n",
    "        else:\n",
    "            deduped_list.append(item)\n",
    "            prev_item = item\n",
    "\n",
    "with open(OUTPUT_JSON_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(deduped_list, fout, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ğŸ‰ å®Œæˆï¼å…±å¤„ç† {len(deduped_list)} æ¡è®°å½•ã€‚è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_JSON_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672f7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1544608/1544608 [02:56<00:00, 8759.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ å®Œæˆï¼å†™å…¥ï¼šrecipes_cleaned111_fixed.jsonï¼ˆå…± 1544608 æ¡è®°å½•ï¼‰\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_FILE = \"recipes_cleaned_dedup.json\"\n",
    "OUTPUT_FILE = \"recipes_cleaned111_fixed.json\"\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# ä¿®å¤ name\n",
    "# -----------------------------\n",
    "def fix_name(item):\n",
    "    name = item.get(\"name\", \"\")\n",
    "    if not name:\n",
    "        return item\n",
    "    # åˆ é™¤æ‰€æœ‰ç©ºæ‹¬å· ()ã€ï¼ˆï¼‰ã€[ ]ã€ã€ ã€‘ï¼Œä¸é™ä½ç½®\n",
    "    name = re.sub(r'\\(\\s*\\)', '', name)\n",
    "    name = re.sub(r'ï¼ˆ\\s*ï¼‰', '', name)\n",
    "    name = re.sub(r'\\[\\s*\\]', '', name)\n",
    "    name = re.sub(r'ã€\\s*ã€‘', '', name)\n",
    "    # å»æ‰å¼€å¤´ç»“å°¾å¼•å·\n",
    "    name = re.sub(r'^[\\'\"â€œâ€â€˜â€™]+', '', name)\n",
    "    name = re.sub(r'[\\'\"â€œâ€â€˜â€™]+$', '', name)\n",
    "    item['name'] = name.strip()\n",
    "    return item\n",
    "\n",
    "# -----------------------------\n",
    "# æ¸…ç† text\n",
    "# -----------------------------\n",
    "def fix_text(item):\n",
    "    text = item.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        return item\n",
    "    # åˆ é™¤æ‰€æœ‰ç©ºæ‹¬å·\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    text = re.sub(r'ï¼ˆ\\s*ï¼‰', '', text)\n",
    "    text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "    text = re.sub(r'ã€\\s*ã€‘', '', text)\n",
    "    # åˆå¹¶å¤šç©ºæ ¼\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    item['text'] = text\n",
    "    return item\n",
    "\n",
    "# -----------------------------\n",
    "# ç»„åˆå¤„ç†\n",
    "# -----------------------------\n",
    "def process(item):\n",
    "    item = fix_name(item)\n",
    "    item = fix_text(item)\n",
    "    return item\n",
    "\n",
    "# -----------------------------\n",
    "# å¹¶è¡Œå¤„ç† + è¿›åº¦æ¡\n",
    "# -----------------------------\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(process)(item) for item in tqdm(data, desc=\"Processing\", ncols=80)\n",
    ")\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ‰ å®Œæˆï¼å†™å…¥ï¼š{OUTPUT_FILE}ï¼ˆå…± {len(results)} æ¡è®°å½•ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c3dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. id=209584, name=(çå¦®æ›²å¥‡ç½‘çº¢æ›²å¥‡)å’–å•¡å°èŠ±é…æ–¹çƒ˜ç„™è§†é¢‘é¥¼å¹²ç¯‡2, text_len=1500\n",
      "2. id=1080742, name=(è§†é¢‘)æ­£å®—é¦™æ¸¯æ›²å¥‡è›‹æŒ ä¸å¡Œé¦…çš„è›‹æŒ, text_len=1500\n",
      "3. id=984950, name=(è¶…è¯¦ç»†)æ–°æ‰‹100æˆåŠŸæ‰‹å·¥é¦’å¤´ç´«è–¯å‘³çº¢æ£å‘³çº¢ç³–å‘³é»‘èŠéº»ç‡•éº¦å‘³ ä¸­ç­‹é¢ç²‰é«˜ç­‹é¢ç²‰, text_len=1500\n",
      "4. id=1022133, name=---ç›´æ’­---å…»å¤©ç„¶é…µç§-åç»­å–‚å…»å‚¨å­˜, text_len=1500\n",
      "5. id=362299, name=-å¤§è™¾ç›–é¥­-ï¼ˆå°é¾™è™¾åšæ³•çš„æ±¤æ±ç‰ˆåŒç†å¯é€šå¾€åšå°é¾™è™¾ï¼‰å‡è‚¥äººç¾¤è§£é¦‹ï¼ä¸åŠ ä¸»é£Ÿå°±æ˜¯ä¸€é“è¶…çº§ä¸‹é…’èœ-å•¤é…’è™¾-, text_len=1500\n",
      "6. id=503724, name=02è‡ªåˆ¶ç†ŸçŒ«é¥­é¸¡èƒ¸çŒªç˜¦è‚‰ç§‹åˆ€é±¼ä¸‰æ–‡é±¼, text_len=1500\n",
      "7. id=1039051, name=0åŸºç¡€0å¤±è´¥çš„å·´æ–¯å…‹çƒ¤èŠå£«è›‹ç³•ï¼ˆé™„èµ å¥¶æ²¹èŠå£«ä¿å­˜æ³•ï¼‰, text_len=1500\n",
      "8. id=104371, name=0åŸºç¡€ä¹Ÿå¯è½»æ¾åšå‡ºçš„æƒ…äººèŠ‚ä¸“å±ç¾é£Ÿ, text_len=1500\n",
      "9. id=822869, name=0åŸºç¡€ä¿å§†çº§ä¸è—ç§ç³»åˆ—(ä¸‰)ä¸ä¼šè£‚çš„æˆšé£è›‹ç³•å·(é™„å¸¦ç½‘çº¢æŠ±æŠ±å·åšæ³•), text_len=1500\n",
      "10. id=1193665, name=0åŸºç¡€ä¿å§†çº§ä¸è—ç§ç³»åˆ—(äºŒ)è‘±æ²¹é¦™é…¥æ˜¥é¥¼(é™„è‘±æ²¹çš„ç†¬åˆ¶æ–¹æ³•), text_len=1500\n",
      "11. id=657645, name=0åŸºç¡€ä¿å§†çº§ä¸è—ç§ç³»åˆ—(å››)æ‰‹å·¥é¦’å¤´ç¯‡, text_len=1500\n",
      "12. id=955188, name=0å¤±è´¥ä¸ç”¨å·å¸˜å¾’æ‰‹å·åˆ¶ç´«èœåŒ…é¥­å¤§æ³•, text_len=1500\n",
      "13. id=739412, name=0å¤±è´¥å†°ç³–å·è´æŸ æª¬è†ï¼ˆæ­¢å’³åŒ–ç—°ï¼‰, text_len=1500\n",
      "14. id=765984, name=0å¤±è´¥æˆšé£è›‹ç³•ï¼Œ8å¯¸ï¼Œé€‚åˆæ‡’äººä¸å°ç™½ï¼Œä¸€æ¬¡å°±èƒ½æˆåŠŸ, text_len=1500\n",
      "15. id=1228040, name=0å¤±è´¥çš„æµå¿ƒè›‹åŒ…é¥­, text_len=1500\n",
      "16. id=1374614, name=0å¤±è´¥é…¸å¥¶å°æº¶è±†ï¼Œé€‚åˆ6ä¸ªæœˆä»¥ä¸Šçš„å®å®åƒï¼ˆæ­¤é…æ–¹åªé€‚åˆå¯¹è›‹ç™½ä¸è¿‡æ•çš„å®å®ï¼‰, text_len=1500\n",
      "17. id=145006, name=0è‰²ç´ ä¸­è¡—1946å½©ç»˜é¾ŸèƒŒå¶é‡ç“£å·¨ç«ç‘°å¡ä»•è¾¾é…±ç‰›ä¹³è›‹ç³•å·, text_len=1500\n",
      "18. id=255929, name=1-4é›†æ—¥å‰§ã€Šé£å¹³æµªé™çš„é—²æš‡ã€‹å…¨éƒ¨ç¾é£Ÿå¤§æ±‡æ€»ï¼ã€Šå‡ªçš„æ–°ç”Ÿæ´»ã€‹å·§å…‹åŠ›ç™¾å¥‡æµæ°´ç´ é¢æ¤°å­èŒ¶é‡‘æªé±¼åå¸é»„æ²¹åœŸè±†é…±æ²¹åœŸè±†å¹´ç³•æ²¹ç‚¸æ²™ä¸é±¼å†°æ·‡æ·‹å—¨æ£’ç”ŸèœåŒ…è‚‰çƒ­éº¦å¥¶, text_len=1500\n",
      "19. id=1270112, name=1.14ä¿®æ”¹ æ‰‹å·¥éº¦èŠ½ç³– é›†åˆäº†ä¸‹å¨æˆ¿å¤šå®¶é«˜åˆ†åˆ¶ä½œæ€»ç»“çš„, text_len=1500\n",
      "20. id=202389, name=1.5æ¬¡å‘é…µ--å¤©ç„¶é…µç§ç³–æ¸æ©™çš®ä¸¹éº¦åå¸, text_len=1500\n",
      "21. id=445559, name=1000gæ·¡å¥¶æ²¹åå¸é¢åŒ…, text_len=1500\n",
      "22. id=89161, name=1000å…‹åå¸é¢åŒ…1000gç‰›å¥¶åå¸é¢åŒ…, text_len=1500\n",
      "23. id=711641, name=100ä¸ªåå¸ç»ƒä¹ , text_len=1500\n",
      "24. id=1278735, name=100ä¸ªæ©„æ¦„æ²¹åå¸å‘é¢åŒ…åŒ äººä»¬è‡´æ•¬ï¼, text_len=1500\n",
      "25. id=604791, name=100ä¸­ç§åŒ—æµ·é“åå¸, text_len=1500\n",
      "26. id=468656, name=100å…¨ç¨‹é¢åŒ…æœºå‡ºè†œå‘é…µçƒ˜çƒ¤æ¤°è“‰é¢åŒ…, text_len=1500\n",
      "27. id=1472169, name=100å…¨éº¦ä¸‰æ˜æ²»åå¸é¢åŒ…Vegan ä½è„‚å¥åº·å‡è‚¥è¶…çº§å¿«æ‰‹, text_len=1500\n",
      "28. id=782346, name=100å…¨éº¦ä¸‰è‰²è—œéº¦åå¸é¢åŒ…å‡è„‚å¿…å¤‡å¥åº·ä¸»é£Ÿä¸‰æ˜æ²», text_len=1500\n",
      "29. id=1032619, name=100å…¨éº¦åå¸, text_len=1500\n",
      "30. id=269412, name=100å…¨éº¦åå¸-ä½ ä¸ºå•¥è¦åƒå…¨éº¦ç²‰, text_len=1500\n",
      "31. id=856558, name=100å…¨éº¦åå¸ï¼ˆæ³¢å…°ç§Â·æ‰‹æ‰ï¼‰, text_len=1500\n",
      "32. id=740974, name=100å…¨éº¦æŠ¹èŒ¶çº¢è±†é¢åŒ…ï¼ˆè‡ªæ‘¸ç´¢è¶…è¯¦ç»†æ­¥éª¤ï¼Œæ–°æ‰‹å‡ºå“ï¼Œå¤§ç¥å‹¿å…¥ï¼‰, text_len=1500\n",
      "33. id=1337909, name=100å…¨éº¦è´æœ, text_len=1500\n",
      "34. id=1438522, name=100å…¨éº¦è´æœ åŸºç¡€çº¯å…¨éº¦è´æœ, text_len=1500\n",
      "35. id=858979, name=100å…¨éº¦é…¸å¥¶åå¸ çº¯å…¨éº¦ æ³¢å…°ç§, text_len=1500\n",
      "36. id=1511450, name=100å…¨éº¦é¢åŒ…(æ— ç³–æ— æ²¹æ— æ·»åŠ )-å°ç¾ç‰ˆ, text_len=1500\n",
      "37. id=1513337, name=100å…¨éº¦é¸¡è›‹åå¸ ç›´æ¥æ³• ç‹åç»†éº¸ç»†ç²’ çº¯å…¨éº¦ä¾ç„¶è½¯åˆ°ä¸‹è…°, text_len=1500\n",
      "38. id=1323245, name=100å…¨éº¦é»‘ç³–é…¸å¥¶åå¸ï¼ˆå†·è—ä¸­ç§æ³•ï¼‰, text_len=1500\n",
      "39. id=1036997, name=100å‡¤æ¢¨é¦…ï¼Œå¥½åƒåˆ°åœä¸ä¸‹æ¥çš„å‡¤æ¢¨é…¥è è, text_len=1500\n",
      "40. id=991237, name=100å‘èŠ½è°·ç‰©åå¸ä¸¨å¥åº·Â·çƒ˜ç„™, text_len=1500\n",
      "41. id=760944, name=100å«æ°´é‡çš„Ciabattaå¤å·´è¾¾å¤å…«è¾¾, text_len=1500\n",
      "42. id=1003983, name=100å«æ°´é‡çš„è°·ç‰©åå¸, text_len=1500\n",
      "43. id=1471763, name=100åœŸå‡¤æ¢¨é…¥, text_len=1500\n",
      "44. id=948673, name=100å¤©ä¸é‡å¤ã€å‡è‚¥ä¿æŒæœŸçš„æ¯æ—¥å¿«æ‰‹ä¾¿å½“è®°å½•æŒç»­æ›´æ–°, text_len=1500\n",
      "45. id=143776, name=100å¤©ä¸é‡æ ·è¯ºå¦ˆçˆ±å¿ƒæ—©é¤, text_len=1500\n",
      "46. id=210629, name=100å¤©å¥åº·å‡è„‚ä¾¿å½“, text_len=1500\n",
      "47. id=1224259, name=100å¤©ç„¶é…µæ¯è´æœ - sourdough bagels, text_len=1500\n",
      "48. id=1082466, name=100å¤©ç„¶é…µç§é²é‚¦ç§æ³•æ£å°ç»“ï¼ˆ18.12-19.05ï¼‰, text_len=1500\n",
      "49. id=540556, name=100æˆåŠŸè¶…æŸ”è½¯çš„æ‹‰ä¸æ—¥å¼åå¸ï¼ˆé¢åŒ…æœºä¸€æ¬¡å‘é…µç‰ˆï¼‰, text_len=1500\n",
      "50. id=1504670, name=100æ— æ²¹è„‚ä½å¡åŠç†ŸèŠå£«ï¼ˆæ— å¥¶æ²¹ã€é»„æ²¹ã€æ¤ç‰©æ²¹ã€å¥¶é…ªï¼‰, text_len=1500\n",
      "51. id=789247, name=100æ— æ²¹è„‚ä½çƒ­é‡å·§å…‹åŠ›è£…é¥°è›‹ç³•ï¼Œå‡è„‚å‹å¥½ï¼ˆé»„æ²¹ã€æ¤ç‰©æ²¹ã€å¥¶æ²¹ã€å¥¶é…ªéƒ½æ²¡æœ‰ï¼ï¼‰, text_len=1500\n",
      "52. id=17449, name=100æ— æ²¹è„‚ä½è„‚æŠ¹èŒ¶è½»ä¹³é…ªè›‹ç³•ï¼ˆæ— å¥¶æ²¹å¥¶é…ªã€é»„æ²¹ã€å¥¶æ²¹ï¼‰, text_len=1500\n",
      "53. id=1118252, name=100æ— æ²¹è„‚æ— æ²¹è›‹ç³•å·ï¼ˆé›¶å¡ç³–ã€å°å››å·æ— æ²¹æ”¹ç‰ˆï¼Œé™„è¶…å¥½åƒ0è„‚ä½å¡å¤¹é¦…ï¼‰, text_len=1500\n",
      "54. id=726541, name=100æ¬¾èŠ±æ ·æ°´æœæ‹¼ç›˜ï¼ˆä¸€ï¼‰, text_len=1500\n",
      "55. id=874376, name=100ç™¾åˆ†ç™¾å…¨éº¦èœ‚èœœé…¸å¥¶åå¸450gæ— æ²¹æŸ”è½¯æ‹‰ä¸ï¼ˆç›´æ¥æ³•é¢åŒ…æœºï¼‰, text_len=1500\n",
      "56. id=875065, name=100ç§èŠ±æ ·æ—©é¤ä¸é‡å¤ï¼ˆé™„è¯¦ç»†çš„æ‘†ç›˜åšæ³•ï¼‰, text_len=1500\n",
      "57. id=1186346, name=100çº¯å…¨éº¦åå¸ ä¸­ç§æ³•, text_len=1500\n",
      "58. id=1156729, name=100çº¯å…¨éº¦åå¸ æ±¤ç§å…¨éº¦, text_len=1500\n",
      "59. id=202113, name=100çº¯å…¨éº¦åå¸ä¹Ÿå¾ˆæŸ”è½¯ï¼æ— ç³–æ— æ²¹ å¥èº«ä¸‰æ˜æ²»é¦–é€‰, text_len=1500\n",
      "60. id=1288726, name=100çº¯å…¨éº¦è—œéº¦åå¸ æ³¢å…°ç§, text_len=1500\n",
      "61. id=982895, name=100çº¯é»‘éº¦åå¸ èŠéº»é»‘éº¦åå¸, text_len=1500\n",
      "62. id=1332904, name=100èéº¦è½¯é¥¼èéº¦é¢çš„Nç§åƒæ³•, text_len=1500\n",
      "63. id=16270, name=100é»‘å…¨éº¦åå¸ï¼ˆæŸ”è½¯å¥åº·è½»æ¾å‡ºè†œï¼‰, text_len=1500\n",
      "64. id=1266791, name=101ç§é²œæ¦¨æœè”¬æ±çš„åšæ³•å¤§å…¨å’ŒåŠŸæ•ˆ-1, text_len=1500\n",
      "65. id=1015396, name=101ç§é²œæ¦¨æœè”¬æ±çš„åšæ³•å¤§å…¨å’ŒåŠŸæ•ˆ-2, text_len=1500\n",
      "66. id=822334, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡---6å¯¸åœ†9å¯¸æ–¹ç›˜æ‹‰èŠ±æˆšé£è›‹ç³•, text_len=1500\n",
      "67. id=253640, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡---ä¸é¡¶ä¸Šç®¡çš„6å¯¸9å¯¸æˆšé£è›‹ç³•ç”Ÿæ—¥è›‹ç³•ä»Šå¤©ä½ æ°”ç–¯äº†å—, text_len=1500\n",
      "68. id=831866, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡---æ‰‹æ‰å·¨å¥½åƒçš„ç»µè½¯æ‹‰ä¸å°é¤åŒ…å°é¢åŒ…, text_len=1500\n",
      "69. id=1544546, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡---æ¶®ç¾Šè‚‰ç»é…è¶…è¯¦ç»†è€åŒ—äº¬èŠéº»é…±çƒ§é¥¼ç«çƒ§, text_len=1500\n",
      "70. id=622668, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡--ä¸å†è‹¦æ¶©çš„ä¼ ç»Ÿæ¡ƒé…¥æåº¦è¿˜åŸå°æ—¶å€™çš„å‘³é“ï¼ˆé™„è¯¦ç»†å®éªŒï¼‰, text_len=1500\n",
      "71. id=1222092, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡--ä¸ç ´ä¸è£‚è›‹ç³•å· åŸå‘³å¥¶æ²¹æœé…±ç«è…¿é¦™è‘±ï¼Œè‡ªç”±å‘æŒ¥, text_len=1500\n",
      "72. id=667549, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡--å¤§çœ¼è›™keroppié’è›™é€ å‹æ’ç”»åå¸é¢åŒ…, text_len=1500\n",
      "73. id=785131, name=10Lå°çƒ¤ç®±çƒ¤ä¸€åˆ‡--ç½‘çº¢é«˜é¢œå€¼ä¸€å‘è¥¿ç“œé€ å‹åå¸é¢åŒ…, text_len=1500\n",
      "74. id=965407, name=10Mä¸æµªè´¹è›‹é»„çš„åŸå‘³æº¶è±†ï¼šå®å®è¾…é£Ÿè¥å…»é£Ÿè°±èœè°±, text_len=1500\n",
      "75. id=1326656, name=10MåŒè‰²å¥¶é¦™å°é¦’å¤´ï¼šå®å®è¾…é£Ÿè¥å…»é£Ÿè°±èœè°±, text_len=1500\n",
      "76. id=549815, name=10ä¸ªæœˆå­é¤é˜¿å¦ˆæ•™è½é£Ÿå¹³D, text_len=1500\n",
      "77. id=1451886, name=10åˆ†é’Ÿå¥åº·ç«è½¦é¤ï¼šåšä¸‰æ˜æ²»æ‰‹æŠ“é¥¼é…¸å¥¶æ°´æœï¼ˆç«è½¦ä¸Šçš„å¸ç›ç¥å™¨ï¼‰, text_len=1500\n",
      "78. id=775075, name=10åˆ†é’Ÿå…¨å®¶ä½ç³–ä½å¡æ—©é¤ å¿«æ‰‹æ¤°å¥¶æ¤°ä¸ã€æ¨±æ¡ƒã€è”“è¶Šè“ã€è“è“ã€æ¡‘è‘šç›èŠ¬è›‹ç³•ï¼ˆMuffinï¼‰, text_len=1500\n",
      "79. id=487723, name=10åˆ†é’Ÿåƒä¸Šé¸¡è›‹çŒé¥¼, text_len=1500\n",
      "80. id=393423, name=10åˆ†é’Ÿå¿«æ‰‹æ—©é¤ç³»åˆ—ï¼ˆæ‰‹æŠ“é¥¼ã€ä¸‰æ˜æ²»ã€çƒ¤ç®±éƒ½è¦æœ‰ï¼‰, text_len=1500\n",
      "81. id=428314, name=10å¯¸åœ†å½¢è›‹ç³•èƒšå’Œæ°´æœè›‹ç³•, text_len=1500\n",
      "82. id=1421248, name=10å¯¸æ™®é€šé¢ç²‰æˆšé£é›€å·¢æ·¡å¥¶æ²¹è›‹ç³•, text_len=1500\n",
      "83. id=1237645, name=10å¯¸é˜³ææ·±ç›˜æˆšé£è›‹ç³•è¯¦ç»†ç‰ˆï¼ˆå¯åšçº¢åŒ…è›‹ç³•èƒšï¼‰ï¼ˆç›’å­è›‹ç³•èƒšï¼‰é€‚åˆç§æˆ¿æ–°æ‰‹æ“ä½œ, text_len=1500\n",
      "84. id=171284, name=10å¯¸é»‘æ¤’çŒªè‚‰æŠ«è¨ï¼ˆå†…é™„æŠ«è¨é…±åšæ³•å’Œè¯¦ç»†æ­¥éª¤ï¼‰, text_len=1500\n",
      "85. id=610021, name=10å¼ å›¾æ•™ä½ å¦‚ä½•ç…åˆ¶ä¸€ç¢—å†°ç³–é“¶è€³, text_len=1500\n",
      "86. id=1339931, name=10ç§å¯ä»¥DIYè‡ªåˆ¶çš„çƒ˜åŸ¹åŸæ–™, text_len=1500\n",
      "87. id=1373316, name=1100 æ³¢å…°ç§äºšéº»ç±½å…¨éº¦åŒ—æµ·é“åå¸, text_len=1500\n",
      "88. id=1459881, name=11å¯¸æˆšé£è›‹ç³•èƒš, text_len=1500\n",
      "89. id=901834, name=11ç§å£å‘³çš„ç²½å­.å’¸ç²½.ç”œç²½-é™„ç²½å­åŒ…æ³•, text_len=1500\n",
      "90. id=821012, name=12.æ¤°è“‰åå¸ï¼ˆå°‘æ²¹å°‘ç³–ï¼‰, text_len=1500\n",
      "91. id=560170, name=12å‡çƒ¤ç®±åšå‡ºçš„6å¯¸æˆšé£è›‹ç³•, text_len=1500\n",
      "92. id=1543862, name=12å¯¸æˆšé£è›‹ç³•é…æ–¹ï¼ˆ9è›‹ï¼‰-é•¿å¸32GS, text_len=1500\n",
      "93. id=1396286, name=12å¹´5000é›¶å¤±è´¥çš„æˆšé£è›‹ç³•é…æ–¹, text_len=1500\n",
      "94. id=627128, name=12æ¬¾èŠ±æ ·é¦’å¤´é›†ç»“å·, text_len=1500\n",
      "95. id=1542612, name=13.è½¯ç»µç»µç™½åå¸ï¼ˆçƒ«ç§æ³•ï¼‰, text_len=1500\n",
      "96. id=478202, name=14.é¦™è‘±è‚‰æ¾åå¸, text_len=1500\n",
      "97. id=487971, name=14ç§å¸¸è§æ„å¤§åˆ©é¢åŠä»–ä»¬çš„æ­£ç¡®é£Ÿç”¨æ–¹å¼, text_len=1500\n",
      "98. id=40884, name=14ç§æ— æ•Œå¥½åƒæ³¡èœçš„åšæ³•, text_len=1500\n",
      "99. id=273523, name=15.æµæ³ªåå¸ï¼ˆå‡¯è¨ç³ç²‰ï¼‰, text_len=1500\n",
      "100. id=1100988, name=1500å¤§å¡ä¸€æ—¥é…é¤ï¼šé…¸ç”œçº¢è–¯è±†æ³¥è¾£è‚ æ—¶è”¬è±†é¥­çƒ¤é¸¡èƒ¸å‘³å™Œæ‹Œé¢, text_len=1500\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ•°æ®æ¡ç›®\n",
    "import json\n",
    "\n",
    "INPUT_FILE = \"recipes_cleaned111_fixed.json\"\n",
    "TOP_K = 100\n",
    "\n",
    "# è¯»å– JSON æ–‡ä»¶\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# è®¡ç®— text é•¿åº¦\n",
    "items_with_len = []\n",
    "for item in data:\n",
    "    text = item.get(\"text\", \"\")\n",
    "    length = len(text)\n",
    "    items_with_len.append((length, item))\n",
    "\n",
    "# æŒ‰é•¿åº¦æ’åº\n",
    "items_with_len.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "# å–å‰ 100 ä¸ª\n",
    "top_items = items_with_len[:TOP_K]\n",
    "\n",
    "# è¾“å‡ºï¼ˆä½ å¯ä»¥æ”¹æˆä¿å­˜åˆ°æ–‡ä»¶ï¼‰\n",
    "for i, (length, item) in enumerate(top_items, 1):\n",
    "    print(f\"{i}. id={item['id']}, name={item['name']}, text_len={length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f97a7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»æ ·æœ¬æ•°ï¼ˆåŒ…æ‹¬è£å‰ªå—ï¼‰: 1539473\n",
      "ä¸åŒé£Ÿç‰©æ•°ï¼ˆæŒ‰ id ç»Ÿè®¡ï¼‰: 1539473\n",
      "ä¸åŒé£Ÿç‰©æ•°ï¼ˆæŒ‰ name ç»Ÿè®¡ï¼‰: 1059441\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"stage2_output.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ç»Ÿè®¡ä¸åŒçš„é£Ÿç‰©æ•°é‡ï¼ˆæ ¹æ® id æˆ– nameï¼‰\n",
    "unique_ids = set(item[\"id\"] for item in data)\n",
    "unique_names = set(item[\"name\"] for item in data)\n",
    "\n",
    "print(f\"æ€»æ ·æœ¬æ•°ï¼ˆåŒ…æ‹¬è£å‰ªå—ï¼‰: {len(data)}\")\n",
    "print(f\"ä¸åŒé£Ÿç‰©æ•°ï¼ˆæŒ‰ id ç»Ÿè®¡ï¼‰: {len(unique_ids)}\")\n",
    "print(f\"ä¸åŒé£Ÿç‰©æ•°ï¼ˆæŒ‰ name ç»Ÿè®¡ï¼‰: {len(unique_names)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae181319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å»é‡æ‰¹æ¬¡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1060/1060 [2:55:00<00:00,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ å®Œæˆï¼æ€»ç»„æ•° 1059441ï¼Œè¾“å‡ºæ–‡ä»¶ï¼šstage1_output_line.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# åœ¨nameç›¸åŒæƒ…å†µä¸‹å¯¹textè¿›è¡Œå»é‡\n",
    "import json\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "import Levenshtein\n",
    "from rapidfuzz.distance import LCSseq\n",
    "\n",
    "# ============================\n",
    "# ç›¸ä¼¼åº¦å‡½æ•°\n",
    "# ============================\n",
    "def edit_similarity(a, b):\n",
    "    max_len = max(len(a), len(b))\n",
    "    if max_len == 0:\n",
    "        return 0\n",
    "    return 1 - Levenshtein.distance(a, b) / max_len\n",
    "\n",
    "split_chars = re.compile(r'[^\\w]+', re.UNICODE)\n",
    "def word_set(s):\n",
    "    return set([w for w in split_chars.split(s) if w])\n",
    "\n",
    "def jaccard(A_set, B_set):\n",
    "    if not A_set and not B_set:\n",
    "        return 0\n",
    "    return len(A_set & B_set) / len(A_set | B_set)\n",
    "\n",
    "def lcs_ratio(a, b):\n",
    "    n = max(len(a), len(b))\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    return LCSseq.normalized_similarity(a, b)\n",
    "\n",
    "def full_similarity(a, b, setA, setB):\n",
    "    return (\n",
    "        0.4 * edit_similarity(a, b) +\n",
    "        0.3 * jaccard(setA, setB) +\n",
    "        0.3 * lcs_ratio(a, b)\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# æ ¸å¿ƒæ–‡æœ¬æŠ½å–\n",
    "# ============================\n",
    "core_re_dish = re.compile(r\"dish[:ï¼š](.*?) recipeIngredient\", re.S)\n",
    "core_re_instr = re.compile(r\"recipeInstructions[:ï¼š](.*?)(keywords|$)\", re.S)\n",
    "core_re_kw = re.compile(r\"keywords[:ï¼š](.*)$\", re.S)\n",
    "\n",
    "def extract_core_text(item):\n",
    "    t = item.get(\"text\", \"\")\n",
    "    d = core_re_dish.search(t)\n",
    "    i = core_re_instr.search(t)\n",
    "    k = core_re_kw.search(t)\n",
    "    res = []\n",
    "    if d: res.append(d.group(1).strip())\n",
    "    if i: res.append(i.group(1).strip())\n",
    "    if k: res.append(k.group(1).strip())\n",
    "    return \" \".join(res)\n",
    "\n",
    "# ============================\n",
    "# å»é‡å‡½æ•°\n",
    "# ============================\n",
    "def deduplicate_same_name_group(group, threshold=0.7):\n",
    "    cached = [{\"item\": item, \"core\": extract_core_text(item), \"set\": word_set(extract_core_text(item))} for item in group]\n",
    "    kept = []\n",
    "    for obj in cached:\n",
    "        a = obj[\"core\"]\n",
    "        aset = obj[\"set\"]\n",
    "        is_dup = False\n",
    "        for k in kept:\n",
    "            score = full_similarity(a, k[\"core\"], aset, k[\"set\"])\n",
    "            if score >= threshold:\n",
    "                is_dup = True\n",
    "                break\n",
    "        if not is_dup:\n",
    "            kept.append(obj)\n",
    "    return [obj[\"item\"] for obj in kept]\n",
    "\n",
    "# ============================\n",
    "# ä¸»æµç¨‹\n",
    "# ============================\n",
    "input_file = \"recipes_cleaned111_fixed.json\"\n",
    "output_file = \"stage1_output_line.json\"\n",
    "save_every = 1000  # æ¯å¤„ç† 1000 ä¸ª group å°±å†™ä¸€æ¬¡\n",
    "\n",
    "# 1. è½½å…¥æ•°æ®\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. æ’åºå¹¶åˆ†ç»„\n",
    "data.sort(key=lambda x: x['name'])\n",
    "groups = [list(g) for _, g in groupby(data, key=lambda x: x[\"name\"])]\n",
    "\n",
    "n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "final_data = []\n",
    "\n",
    "# 3. æŒ‰ batch line-by-line å¢é‡å¤„ç†\n",
    "for i in tqdm(range(0, len(groups), save_every), desc=\"å»é‡æ‰¹æ¬¡\", ncols=90):\n",
    "    batch = groups[i:i+save_every]\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(deduplicate_same_name_group)(group) for group in batch\n",
    "    )\n",
    "    # flatten\n",
    "    batch_flat = [item for sub in results for item in sub]\n",
    "    final_data.extend(batch_flat)\n",
    "\n",
    "    # å¢é‡ä¿å­˜åˆ°æ–°æ–‡ä»¶\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ‰ å®Œæˆï¼æ€»ç»„æ•° {len(groups)}ï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c5e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…³é”®è¯ç›¸ä¼¼åº¦å»é‡å®Œæˆï¼Œæ€»æ¡ç›®ï¼š 9990\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c12f0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹å¤„ç† 1539473 æ¡è®°å½•ï¼ˆå¹¶è¡Œ: 23 æ ¸å¿ƒï¼Œæ¯ 1000 æ¡å¢é‡ä¿å­˜ï¼‰...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†æ‰¹æ¬¡:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1442/1540 [6:18:52<89:18:17, 3280.59s/it]c:\\Users\\14480\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "å¤„ç†æ‰¹æ¬¡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1540/1540 [10:27:30<00:00, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œæˆï¼å…±å¤„ç† 1539473 æ¡è®°å½•ï¼Œå·²ä¿å­˜åˆ° stage2_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- åŸºç¡€å·¥å…·å‡½æ•° ----------------\n",
    "def lev_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return lev_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    prev = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            ins = prev[j + 1] + 1\n",
    "            dele = curr[j] + 1\n",
    "            sub = prev[j] + (c1 != c2)\n",
    "            curr.append(min(ins, dele, sub))\n",
    "        prev = curr\n",
    "    return prev[-1]\n",
    "\n",
    "def lcs_length(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            dp[i+1][j+1] = dp[i][j]+1 if a[i]==b[j] else max(dp[i][j+1], dp[i+1][j])\n",
    "    return dp[n][m]\n",
    "\n",
    "def similarity(a: str, b: str) -> float:\n",
    "    max_len = max(len(a), len(b))\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "    ed = lev_distance(a, b)\n",
    "    edit_sim = 1 - ed / max_len\n",
    "    set_a, set_b = set(a), set(b)\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    jaccard = inter / union if union else 0\n",
    "    lcs = lcs_length(a, b)\n",
    "    lcs_ratio = lcs / max_len\n",
    "    return 0.4*edit_sim + 0.3*jaccard + 0.3*lcs_ratio\n",
    "\n",
    "def deduplicate_keywords(keywords, threshold=0.75):\n",
    "    kept = []\n",
    "    for kw in keywords:\n",
    "        if not kept:\n",
    "            kept.append(kw)\n",
    "            continue\n",
    "        if any(similarity(kw, k) >= threshold for k in kept):\n",
    "            continue\n",
    "        kept.append(kw)\n",
    "    return kept\n",
    "\n",
    "def split_keywords(raw: str):\n",
    "    \"\"\"æ‹†åˆ† keywords å­—ç¬¦ä¸²å¹¶è¿‡æ»¤ç©ºå€¼å’Œ description å†…å®¹\"\"\"\n",
    "    tokens = [k.strip() for k in re.split(r\"[ ,ï¼Œ;ï¼›]+\", raw) if k.strip()]\n",
    "    tokens = [t for t in tokens if not t.lower().startswith(\"description\")]\n",
    "    return tokens\n",
    "\n",
    "def extract_keywords_from_text(text: str):\n",
    "    \"\"\"ä» text ä¸­ç²¾å‡†æå– keywords éƒ¨åˆ†\"\"\"\n",
    "    match = re.search(r\"keywords[:ï¼š](.*?)(?=description|recipeInstructions|$)\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def remove_keywords_from_text(text: str):\n",
    "    \"\"\"\n",
    "    åˆ é™¤ text ä¸­çš„ keywords æ®µï¼Œå¹¶å»æ‰æ¢è¡Œç¬¦å’Œæœ«å°¾å¤šä½™çš„ 'description:'\n",
    "    \"\"\"\n",
    "    # åˆ é™¤ keywords æ®µ\n",
    "    text = re.sub(r\"keywords[:ï¼š].*?(?=description|recipeInstructions|$)\", \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    # æ›¿æ¢æ¢è¡Œç¬¦\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    # åˆ é™¤æœ«å°¾å¤šä½™çš„ description:\n",
    "    text = re.sub(r\"description[:ï¼š]\\s*$\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------------- å¤„ç†å•æ¡è®°å½• ----------------\n",
    "def process_item(item):\n",
    "    # ä» text æå– keywords\n",
    "    raw_kw = extract_keywords_from_text(item.get(\"text\", \"\"))\n",
    "    keywords = split_keywords(raw_kw)\n",
    "    item[\"keywords\"] = deduplicate_keywords(keywords)\n",
    "\n",
    "    # æ›´æ–° textï¼Œåˆ é™¤åŸ keywords æ®µå¹¶å»æ‰æ¢è¡Œç¬¦\n",
    "    item[\"text\"] = remove_keywords_from_text(item.get(\"text\", \"\"))\n",
    "\n",
    "    return item\n",
    "\n",
    "# ---------------- ä¸»æµç¨‹ ----------------\n",
    "input_file = \"stage1_output_line.json\"\n",
    "output_file = \"stage2_output.json\"\n",
    "batch_size = 1000\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "n = len(data)\n",
    "n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "final_data = []\n",
    "\n",
    "print(f\"å¼€å§‹å¤„ç† {n} æ¡è®°å½•ï¼ˆå¹¶è¡Œ: {n_jobs} æ ¸å¿ƒï¼Œæ¯ {batch_size} æ¡å¢é‡ä¿å­˜ï¼‰...\")\n",
    "\n",
    "for start in tqdm(range(0, n, batch_size), desc=\"å¤„ç†æ‰¹æ¬¡\", ncols=90):\n",
    "    batch = data[start:start + batch_size]\n",
    "    processed_batch = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_item)(item) for item in batch\n",
    "    )\n",
    "    final_data.extend(processed_batch)\n",
    "\n",
    "    # å¢é‡ä¿å­˜\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"å®Œæˆï¼å…±å¤„ç† {len(final_data)} æ¡è®°å½•ï¼Œå·²ä¿å­˜åˆ° {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce41c54",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
