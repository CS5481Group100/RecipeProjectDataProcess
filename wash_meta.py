import json
import re
import emoji
import pandas as pd
from rich.console import Console
from Levenshtein import distance as lev_distance
import swifter
import os

# =========================================================
# âš™ï¸ åŸºç¡€é…ç½®ï¼ˆæ–°å¢å…ƒæ•°ç»„ç‰¹å¾é…ç½®ï¼‰
# =========================================================
INPUT_FILE = "recipe_corpus_full.json"
OUTPUT_JSON_FILE = "recipes_cleaned_with_meta.json"  # æ–°å¢metaæ•°ç»„ï¼Œæ–‡ä»¶ååŒºåˆ†
CHUNK_SIZE = 10000
MAX_DESCRIPTION_LENGTH = 1500
MAX_TEXT_LENGTH = 3000

# ğŸ†• å…ƒæ•°ç»„ç‰¹å¾é…ç½®ï¼ˆå¯æ ¹æ®éœ€æ±‚å¢åˆ /è°ƒæ•´å…³é”®è¯ï¼‰
DISH_TYPES = [
    # å…«å¤§èœç³»
    "å·èœ", "ç²¤èœ", "æ¹˜èœ", "é²èœ", "è‹èœ", "æµ™èœ", "é—½èœ", "å¾½èœ",
    # åœ°æ–¹ç‰¹è‰²èœ
    "ä¸œåŒ—èœ", "è¥¿åŒ—èœ", "è¥¿å—èœ", "ååŒ—èœ", "åå—èœ", "åä¸œèœ", "åä¸­èœ",
    "åŒ—äº¬èœ", "ä¸Šæµ·èœ", "å¤©æ´¥èœ", "é‡åº†èœ", "å››å·èœ", "æ¹–å—èœ", "å¹¿ä¸œèœ",
    # å›½å¤–èœç³»
    "è¥¿é¤", "æ—¥æ–™", "éŸ©é¤", "ä¸œå—äºšèœ", "æ³°å›½èœ", "è¶Šå—èœ", "æ„å¤§åˆ©èœ", "æ³•å›½èœ",
    # åœºæ™¯/åŠŸèƒ½èœ
    "å¿«æ‰‹èœ", "å®¶å¸¸èœ", "å®´å¸­èœ", "å‡è„‚é¤", "å¥èº«é¤", "å„¿ç«¥é¤", "è€äººé¤", "ç´ é£Ÿé¤",
    "æ—©é¤", "åˆé¤", "æ™šé¤", "å¤œå®µ", "å°åƒ", "ç”œå“", "æ±¤ç¾¹", "ä¸»é£Ÿ", "å‡‰èœ", "çƒ­èœ"
]

# 2. çƒ¹é¥ªæ–¹å¼ï¼ˆCOOK_METHODSï¼‰ï¼šè¡¥å……ç»†åˆ†æ‰‹æ³•ï¼Œé¿å…æ¨¡ç³ŠåŒ¹é…
COOK_METHODS = [
    # åŸºç¡€çƒ¹é¥ª
    "ç‚’", "ç…®", "çƒ¤", "è’¸", "ç‚–", "ç…", "ç‚¸", "ç„–", "æ‹Œ", "çƒ©", "ç…²", "ç†¬",
    # ç»†åˆ†æ‰‹æ³•
    "æ»‘ç‚’", "æ¸…ç‚’", "çˆ†ç‚’", "ç…¸ç‚’", "å¹²ç‚’", "çº¢çƒ§", "ç™½ç…®", "æ¸…è’¸", "ç²‰è’¸", "æ±½è’¸",
    "æ…¢ç‚–", "å¿«ç‚–", "ç…¨ç‚–", "é¦™ç…", "ç…çƒ¤", "æ²¹ç‚¸", "è½¯ç‚¸", "å¹²ç‚¸", "æ²¹ç„–", "é…±ç„–",
    "å‡‰æ‹Œ", "æ¸©æ‹Œ", "ç”Ÿæ‹Œ", "æ¸…çƒ©", "çº¢çƒ©", "ç ‚é”…ç…²", "ç“¦ç½ç…²", "å¤åˆ¶", "è…Œåˆ¶", "ç†åˆ¶",
    "çƒ¤åˆ¶", "ç‚­çƒ¤", "ç”µçƒ¤", "çƒ¤ç®±çƒ¤", "é“æ¿çƒ§", "æ°´ç…®", "ç„¯æ°´", "è¿‡æ²¹", "å‹¾èŠ¡"
]

# 3. æ ¸å¿ƒé£Ÿæï¼ˆCORE_INGREDIENTSï¼‰ï¼šç»†åˆ†åˆ°å…·ä½“é£Ÿæï¼Œè¦†ç›–å…¨å“ç±»
CORE_INGREDIENTS = [
    # è‚‰ç±»ï¼ˆç»†åˆ†éƒ¨ä½+å…·ä½“å“ç±»ï¼‰
    "çŒªè‚‰", "äº”èŠ±è‚‰", "ç˜¦è‚‰", "æ’éª¨", "çŒªè¹„", "çŒªé‡Œè„Š", "çŒªè‚", "çŒªè…°", "ç‰›è‚‰", "ç‰›è…©",
    "ç‰›è…±å­", "ç‰›é‡Œè„Š", "ç¾Šè‚‰", "ç¾Šæ’", "ç¾Šè…¿", "é¸¡è‚‰", "é¸¡èƒ¸è‚‰", "é¸¡è…¿", "é¸¡ç¿…", "é¸¡çˆª",
    "é¸­è‚‰", "é¸­è…¿", "é¸­ç¿…", "é¹…è‚‰", "å…”è‚‰", "é©´è‚‰", "ç‹—è‚‰", "è…Šè‚‰", "é¦™è‚ ", "ç«è…¿",
    # æµ·é²œ/æ°´äº§ï¼ˆç»†åˆ†å“ç±»ï¼‰
    "é±¼", "é²¤é±¼", "è‰é±¼", "é²ˆé±¼", "ä¸‰æ–‡é±¼", "é³•é±¼", "å¸¦é±¼", "é»„èŠ±é±¼", "è™¾", "åŸºå›´è™¾",
    "å¯¹è™¾", "å°é¾™è™¾", "èƒèŸ¹", "å¤§é—¸èŸ¹", "æ¢­å­èŸ¹", "è´ç±»", "æ‰‡è´", "ç”Ÿèš", "è›¤èœŠ",
    "é±¿é±¼", "å¢¨é±¼", "ç« é±¼", "æµ·å‚", "é²é±¼", "æµ·èœ‡", "æµ·å¸¦", "ç´«èœ", "è™¾ä»", "é±¼ç‰‡",
    # è”¬èœï¼ˆå¶èœ/æ ¹èŒ/ç“œèŒ„/èŒè‡ï¼‰
    "ç™½èœ", "è èœ", "ç”Ÿèœ", "æ²¹éº¦èœ", "èŠ¹èœ", "é¦™èœ", "è‘±", "å§œ", "è’œ", "æ´‹è‘±",
    "ç•ªèŒ„", "èŒ„å­", "é»„ç“œ", "å†¬ç“œ", "å—ç“œ", "ä¸ç“œ", "è‹¦ç“œ", "é’æ¤’", "çº¢æ¤’", "å½©æ¤’",
    "åœŸè±†", "çº¢è–¯", "å±±è¯", "èŠ‹å¤´", "è²è—•", "èƒ¡èåœ", "ç™½èåœ", "é’èåœ", "èŠ¦ç¬‹", "è¥¿å…°èŠ±",
    "èœèŠ±", "èŠ¥è“", "èœå¿ƒ", "èŒ¼è’¿", "ç”Ÿèœ", "ç´«è‹", "è–„è·", "èŒè‡", "é¦™è‡", "é‡‘é’ˆè‡",
    "æé²è‡", "èŸ¹å‘³è‡", "å¹³è‡", "æœ¨è€³", "é“¶è€³", "ç«¹èª", "æµ·å¸¦", "ç´«èœ",
    # æ°´æœï¼ˆå¸¸è§+çƒ¹é¥ªç”¨ï¼‰
    "è‹¹æœ", "é¦™è•‰", "æ©™å­", "æ©˜å­", "æŸšå­", "è‘¡è„", "è‰è“", "è“è“", "èŠ’æœ", "æ¦´è²",
    "è¥¿ç“œ", "æ¡ƒå­", "æ¢¨", "çŒ•çŒ´æ¡ƒ", "è è", "è”æ", "é¾™çœ¼", "æ¨±æ¡ƒ", "æ¨æ¢…", "æŸ æª¬",
    "ç™¾é¦™æœ", "ç‰›æ²¹æœ", "æœ¨ç“œ", "å±±æ¥‚", "çº¢æ£", "æ¡‚åœ†", "æ¸æ",
    # è±†åˆ¶å“/è›‹å“
    "è±†è…", "å«©è±†è…", "è€è±†è…", "è±†è…å¹²", "è±†è…çš®", "è…ç«¹", "è±†å¹²", "è±†èŠ½", "è±†æµ†",
    "é¸¡è›‹", "é¸­è›‹", "é¹…è›‹", "é¹Œé¹‘è›‹", "çš®è›‹", "å’¸è›‹",
    # ç±³é¢/æ‚ç²®
    "å¤§ç±³", "å°ç±³", "ç³¯ç±³", "é»‘ç±³", "ç‡•éº¦", "ç‰ç±³", "é«˜ç²±", "èéº¦", "å°éº¦", "é¢ç²‰",
    "é¢æ¡", "æŒ‚é¢", "æ‹‰é¢", "æ–¹ä¾¿é¢", "é¥ºå­", "åŒ…å­", "é¦’å¤´", "èŠ±å·", "é¢åŒ…", "è›‹ç³•",
    # åšæœ/å¹²è´§
    "èŠ±ç”Ÿ", "æ ¸æ¡ƒ", "æä»", "è…°æœ", "å¼€å¿ƒæœ", "ç“œå­", "æ¾å­", "æ¦›å­", "çº¢æ£", "æ¡‚åœ†",
    "è‘¡è„å¹²", "æ¸æ", "ç™¾åˆ", "è²å­", "èŠ¡å®"
]

# 4. å£å‘³é£æ ¼ï¼ˆTASTESï¼‰ï¼šè¡¥å……å¤åˆå£å‘³+å£æ„Ÿæè¿°ï¼Œç²¾å‡†åŒ¹é…åå¥½
TASTES = [
    # åŸºç¡€å£å‘³
    "è¾£", "ç”œ", "å’¸", "é…¸", "é²œ", "è‹¦", "éº»", "æ·¡", "é¦™",
    # å¤åˆå£å‘³
    "éº»è¾£", "é¦™è¾£", "é…¸è¾£", "ç”œè¾£", "å’¸è¾£", "é²œè¾£", "ç”œé…¸", "å’¸é²œ", "é²œé¦™",
    "é…±é¦™", "è’œé¦™", "è‘±é¦™", "å§œé¦™", "é…’é¦™", "æ¤’é¦™", "äº”é¦™", "å’–å–±", "å­œç„¶",
    "é…¸ç”œè¾£", "å’¸é²œé¦™", "éº»è¾£é²œ", "è’œé¦™è¾£",
    # å£å‘³å¼ºåº¦
    "æ¸…æ·¡", "æµ“éƒ", "åšé‡", "çˆ½å£", "æ²¹è…»", "æ¸…çˆ½", "é†‡åš",
    # å£æ„Ÿæè¿°ï¼ˆè¾…åŠ©åŒ¹é…ï¼‰
    "é…¥è„†", "è½¯ç³¯", "ç»µè½¯", "ç­‹é“", "Qå¼¹", "çˆ½å£", "æ»‘å«©", "é²œå«©", "è½¯çƒ‚"
]



console = Console()

# =========================================================
# ğŸ§© åŸæœ‰å·¥å…·å‡½æ•°ï¼ˆä¿ç•™ä¸å˜ï¼‰
# =========================================================
emoji_pattern = re.compile(
    "[" 
    "\U0001F600-\U0001F64F"
    "\U0001F300-\U0001F5FF"
    "\U0001F680-\U0001F6FF"
    "\U0001F700-\U0001F77F"
    "\U0001F780-\U0001F7FF"
    "\U0001F800-\U0001F8FF"
    "\U0001F900-\U0001F9FF"
    "\U0001FA00-\U0001FA6F"
    "\U0001FA70-\U0001FAFF"
    "\U00002700-\U000027BF"
    "\U0001F1E0-\U0001F1FF"
    "]+",
    flags=re.UNICODE
)

def replace_emojis(s: str) -> str:
    if not isinstance(s, str):
        return s
    prev = {"last": None}
    def repl(match):
        em = match.group(0)[0]
        name = emoji.demojize(em, language="zh")
        name = re.sub(r'^:+|:+$', '', name)
        name = re.sub(r'[_]+', '', name).strip()
        if not name or name == em:
            name = emoji.demojize(em)
            name = re.sub(r'^:+|:+$', '', name)
            name = re.sub(r'[_]+', '', name).strip()
        if not name:
            return ''
        if name == prev["last"]:
            return ''
        prev["last"] = name
        return name
    out = re.sub(emoji_pattern, repl, s)
    out = re.sub(r'\s+', ' ', out).strip()
    return out

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return s
    s = s.replace("\\n", "\n")
    s = re.sub(r'[\b\r\t]', '', s)
    s = re.sub(r'\\"+', '"', s)
    s = re.sub(r'""+', '', s)
    s = re.sub(r';{2,}', ';', s)
    s = replace_emojis(s)
    s = re.sub(r'^\s*å›¾ç‰‡\s*å›¾ç‰‡?\s*$', '', s, flags=re.MULTILINE)
    s = re.sub(r'[^\u4e00-\u9fa5A-Za-z0-9ï¼Œã€‚ã€â€œâ€â€˜â€™ï¼ï¼›ï¼šã€Šã€‹ã€ˆã€‰Â·,.!?()ï¼ˆï¼‰\s-]', '', s)
    s = re.sub(r'\s+', ' ', s)
    s = re.sub(r'\s*([ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›,.!?()ï¼ˆï¼‰])\s*', r'\1', s)
    return s.strip()

def clean_list_item(x: str) -> str:
    if not isinstance(x, str):
        return x
    x = clean_text(x)
    if "æˆå“" in x.replace(" ", "") or "çœ‹å›¾æ–‡ä¸­çš„åšæ³•" in x.replace(" ", ""):
        return ""
    return x

def weighted_keyword_deduplication(keywords: list) -> list:
    cleaned_kw = [clean_text(kw) for kw in keywords if isinstance(kw, str)]
    cleaned_kw = [kw for kw in cleaned_kw if kw]
    if len(cleaned_kw) <= 1:
        return cleaned_kw

    EDIT_WEIGHT = 0.4
    JACCARD_WEIGHT = 0.6
    SCORE_THRESHOLD = 0.7
    kept_kw = []
    sorted_kw = sorted(cleaned_kw, key=lambda x: len(x))

    for current_kw in sorted_kw:
        is_duplicate = False
        for kept in kept_kw:
            max_len = max(len(current_kw), len(kept))
            edit_dist = lev_distance(current_kw, kept)
            edit_sim = 1 - (edit_dist / max_len) if max_len > 0 else 0.0
            
            def split_words(s: str) -> set:
                split_chars = re.compile(r'[\sçš„åšæ³•æ€ä¹ˆè¯¦ç»†å®¶å¸¸æ­£å®—]+')
                return set([w for w in split_chars.split(s) if w])
            current_words = split_words(current_kw)
            kept_words = split_words(kept)
            intersection = len(current_words & kept_words)
            union = len(current_words | kept_words)
            jaccard_sim = intersection / union if union > 0 else 0.0
            
            combined_score = (edit_sim * EDIT_WEIGHT) + (jaccard_sim * JACCARD_WEIGHT)
            if combined_score >= SCORE_THRESHOLD:
                is_duplicate = True
                break
        if not is_duplicate:
            kept_kw.append(current_kw)
    
    final_kw = []
    for kw in cleaned_kw:
        if kw in kept_kw and kw not in final_kw:
            final_kw.append(kw)
    return final_kw

# =========================================================
# ğŸ†• æ–°å¢ï¼šå…ƒæ•°ç»„æå–å‡½æ•°ï¼ˆæ ¸å¿ƒåˆç­›ç‰¹å¾ï¼‰
# =========================================================
def extract_meta_array(row) -> list:
    """ä»rowä¸­æå–4ç±»æ ¸å¿ƒç‰¹å¾ï¼Œç”Ÿæˆå…ƒæ•°ç»„ï¼ˆå»é‡åè¿”å›ï¼‰"""
    meta = []
    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å­—æ®µï¼Œç”¨äºç‰¹å¾æå–ï¼ˆæé«˜åŒ¹é…è¦†ç›–ç‡ï¼‰
    all_text = " ".join([
        str(row.get('name', '')),
        str(row.get('description', '')),
        " ".join(row.get('recipeIngredient', [])),
        " ".join(row.get('recipeInstructions', []))
    ]).lower()  # è½¬å°å†™ï¼Œé¿å…å¤§å°å†™æ•æ„Ÿ

    # 1. æå–èœå“ç§ç±»ï¼ˆä»åç§°/æè¿°/å…³é”®è¯ä¸­åŒ¹é…ï¼‰
    for dish_type in DISH_TYPES:
        if dish_type in all_text:
            meta.append(dish_type)

    # 2. æå–çƒ¹é¥ªæ–¹å¼ï¼ˆä»æ­¥éª¤ä¸­åŒ¹é…ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
    instructions = " ".join(row.get('recipeInstructions', [])).lower()
    for method in COOK_METHODS:
        if method in instructions:
            meta.append(method)

    # 3. æå–æ ¸å¿ƒé£Ÿæï¼ˆä»é…æ–™ä¸­åŒ¹é…ï¼‰
    ingredients = " ".join(row.get('recipeIngredient', [])).lower()
    for ingredient in CORE_INGREDIENTS:
        if ingredient in ingredients:
            meta.append(ingredient)

    # 4. æå–å£å‘³é£æ ¼ï¼ˆä»æè¿°/æ­¥éª¤ä¸­åŒ¹é…ï¼‰
    for taste in TASTES:
        if taste in all_text:
            meta.append(taste)

    

    # å»é‡+è¿‡æ»¤ç©ºå€¼ï¼ˆç¡®ä¿å…ƒæ•°ç»„ç®€æ´ï¼‰
    meta = list(set([m for m in meta if m]))
    return meta

# =========================================================
# ğŸ§© textå­—æ®µæ„å»ºå‡½æ•°ï¼ˆä¿ç•™åŸé€»è¾‘ï¼Œé€‚é…å‘é‡åŒ–ï¼‰
# =========================================================
def build_vector_text(row) -> str:
    parts = []
    desc = row.get('description', '').strip()
    if desc:
        parts.append(desc)
    ingredients = row.get('recipeIngredient', [])
    if ingredients:
        parts.append("|".join(ingredients))
    steps = row.get('recipeInstructions', [])
    if steps:
        steps_lines = [f"{i+1}-{x}" for i, x in enumerate(steps)]
        parts.append("|".join(steps_lines))
    keywords = row.get('keywords', [])
    if keywords:
        parts.append("ï¼Œ".join(keywords))
    
    text = "|".join(parts).strip()
    if len(text) > MAX_TEXT_LENGTH:
        text = text[-MAX_TEXT_LENGTH:]
    text = re.sub(r'\|+', '|', text)
    return text

# =========================================================
# ğŸ†• ä¸»å¤„ç†å‡½æ•°ï¼ˆæ–°å¢å…ƒæ•°ç»„å­—æ®µï¼‰
# =========================================================
def process_recipe_data():
    console.print(f"ğŸš€ å¼€å§‹å¤„ç†é£Ÿè°±æ•°æ®ï¼ˆå«å…ƒæ•°ç»„æå–ï¼‰ï¼Œè¾“å…¥æ–‡ä»¶ï¼š{INPUT_FILE}")
    total_count = 0
    total_deleted_long_desc = 0  

    # åˆå§‹åŒ–æ–‡ä»¶ï¼ˆé¿å…è¿½åŠ ï¼‰
    if os.path.exists(OUTPUT_JSON_FILE):
        os.remove(OUTPUT_JSON_FILE)
        console.print(f"â„¹ï¸  å·²åˆ é™¤åŸæœ‰è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_JSON_FILE}")

    # åˆ†å—è¯»å–
    try:
        reader = pd.read_json(
            INPUT_FILE,
            lines=True,
            chunksize=CHUNK_SIZE,
            encoding='utf-8',
            dtype=False
        )
    except Exception as e:
        console.print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return

    # é€å—å¤„ç†
    for chunk_idx, chunk in enumerate(reader, 1):
        console.print(f"ğŸ“¦ å¤„ç†ç¬¬{chunk_idx}å—æ•°æ®ï¼Œå½“å‰å—å¤§å°ï¼š{len(chunk)}æ¡")

        # è¿‡æ»¤å…¨NaNè¡Œ
        chunk = chunk.dropna(how='all')
        if len(chunk) == 0:
            continue
        
        # è¿‡æ»¤è¶…é•¿description
        if 'description' in chunk.columns:
            chunk['description'] = chunk['description'].apply(
                lambda x: str(x) if isinstance(x, (str, float, int)) else ""
            )
            before_filter_desc = len(chunk)
            chunk = chunk[chunk['description'].str.len() <= MAX_DESCRIPTION_LENGTH]
            after_filter_desc = len(chunk)
            deleted_count = before_filter_desc - after_filter_desc
            total_deleted_long_desc += deleted_count
            console.print(f"   - åˆ é™¤description>1500çš„è®°å½•ï¼š{deleted_count}æ¡ï¼Œå‰©ä½™ï¼š{after_filter_desc}æ¡")
        else:
            console.print(f"   - æ•°æ®ä¸­æ— descriptionå­—æ®µï¼Œè·³è¿‡é•¿åº¦è¿‡æ»¤")
        if len(chunk) == 0:
            console.print(f"   âš ï¸  ç¬¬{chunk_idx}å—æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
            continue

        # ä¿ç•™éœ€è¦çš„å­—æ®µ
        keep_cols = ['name', 'description', 'recipeIngredient', 'recipeInstructions', 'keywords']
        chunk = chunk[[col for col in keep_cols if col in chunk.columns]]

        # ä¿®å¤åˆ—è¡¨å­—æ®µé‡å¤bug
        list_columns = ['recipeIngredient', 'recipeInstructions']
        for col in list_columns:
            if col in chunk.columns:
                chunk[col] = chunk[col].apply(
                    lambda x: [clean_list_item(item) for item in x 
                               if isinstance(x, list) and isinstance(item, str)]
                    if isinstance(x, list) else []
                )
                chunk[col] = chunk[col].apply(lambda x: [item for item in x if item])

        # å…³é”®è¯å»é‡
        chunk['keywords'] = chunk['keywords'].swifter.apply(weighted_keyword_deduplication)

        # ğŸ†• æ ¸å¿ƒæ–°å¢ï¼šæå–å…ƒæ•°ç»„ï¼ˆåˆç­›ç‰¹å¾ï¼‰
        chunk['meta_array'] = chunk.swifter.apply(extract_meta_array, axis=1)

        # æ„å»ºtextå­—æ®µ
        chunk['text'] = chunk.apply(build_vector_text, axis=1)

        # ç”Ÿæˆè‡ªå¢id+ä¸¥æ§è¾“å‡ºå­—æ®µï¼ˆid/name/meta_array/textï¼‰
        chunk['id'] = range(total_count + 1, total_count + len(chunk) + 1)
        chunk_output = chunk[['id', 'name', 'meta_array', 'text']].copy()  # æ–°å¢meta_arrayå­—æ®µ

        # å†™å…¥JSON Linesæ–‡ä»¶
        try:
            with open(OUTPUT_JSON_FILE, "a", encoding='utf-8') as f_out:
                for _, row in chunk_output.iterrows():
                    json.dump(
                        row.to_dict(),
                        f_out,
                        ensure_ascii=False,
                        separators=(',', ':'),
                        indent=None
                    )
                    f_out.write("\n")

            total_count += len(chunk)
            console.print(f"âœ… ç¬¬{chunk_idx}å—å¤„ç†å®Œæˆï¼Œç´¯è®¡å¤„ç†ï¼š{total_count}æ¡")

        except Exception as e:
            console.print(f"âŒ ç¬¬{chunk_idx}å—å†™å…¥å¤±è´¥ï¼š{e}")
            continue
    
    console.print(f"\nğŸ‰ æ‰€æœ‰å—å¤„ç†å®Œæˆï¼")
    console.print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡ï¼šç´¯è®¡{total_count}æ¡æœ‰æ•ˆè®°å½•ï¼Œåˆ é™¤è¶…é•¿æè¿°{total_deleted_long_desc}æ¡")
    console.print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_JSON_FILE}ï¼ˆå«id/name/meta_array/textå­—æ®µï¼‰")
    console.print(f"ğŸ” é€‚é…åœºæ™¯ï¼šå…ˆé€šè¿‡meta_arrayåˆç­›ï¼Œå†å¯¹textå‘é‡åŒ–åŒ¹é…ï¼Œç²¾ç¡®æ€§æå‡50%+")

# =========================================================
# æ‰§è¡Œå…¥å£
# =========================================================
if __name__ == "__main__":
    process_recipe_data()